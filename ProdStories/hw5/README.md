### Запуск

1. Install requirements:
`pip install -r requirements.txt`
2. Run training
`python ppo_example.py`

### Награда
```code
if info['step'] == 1:
    self.respawn_explored = info["total_explored"] - info['new_explored']

if info['moved']:
    # не врезались
    if info['new_explored'] > 0:
        # хотим в конце быстро находить пропущенные клетки,
        # но не хотим награждать себя за открытые клетки, если ничего не открыли
        # также не будем себя хвалить за первый степ
        # если не открыли новых
        # (сложно подобрать коэффициенты)
        reward = 0.1 * reward + 5.0 * (info["total_explored"] - self.respawn_explored) / (info["total_cells"] - self.respawn_explored)
    if info['is_new']:
        # предпочитаем переходить к новым клеткам
        # чем крутится на тех, на которых уже были
        reward += 0.1
    # хотим быстрее пройти, поэтому штрафуем за действия
    if action == 1:
        # предпочитаем идти вперед, а не повороты
        reward -= 0.1
    else:
        reward -= 0.5
else:
    # врезались
    reward = -1
```

### Логирование
Для сбора статистик обучения использовался [wandb](https://wandb.ai/shytea/ProdStories?workspace=user-shytea)

### Исследования
В качестве модели было выбрано ppo.

Есть ppo с награждением (ppo_w_is_new) и без награждения (ppo_wo_is_new) за ход на новую клетку, цель заключалась в том, чтобы агент, который разведал территорию вокруг себя, двигался в сторону новых клеток, а не крутился на старых.
Это идея не принесла значимого прироста качества.

Просто ppo - это ppo без награждения за ход и с другими коэффициентами при подсчете reward

Замечание: большое значение имеет правильно подобранные коэффициенты при подсчете реварда, это видно из большой разницы ppo и ppo_wo_is_new, но чтобы подобрать оптимальные, нужно много мощностей.

